{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/github/RiboVsPolyA/examples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#Author: Ioannis Anastopoulos\n",
    "from sklearn.metrics import r2_score,confusion_matrix, accuracy_score,roc_curve,auc,precision_recall_fscore_support,f1_score, average_precision_score\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
    "\n",
    "def F_score(Y_true,predict_classes ,classes=None, plot=False,average=None):\n",
    "    if classes is None:\n",
    "        classes=range(len(set(Y_true)))\n",
    "    eval_metrics_matrix=precision_recall_fscore_support(Y_true,predict_classes,labels=classes,average=average)\n",
    "    f_score_df=pd.DataFrame(eval_metrics_matrix[2], columns=['F_score'],index=classes)\n",
    "    f_score_df=f_score_df.drop_duplicates()\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax=f_score_df.plot(kind='bar', figsize=(7,10))\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "        plt.title('F1_score')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return f_score_df\n",
    "\n",
    "def confusion(Y_true,predict_classes,classes=None,plot=False, title='Confusion Matrix',fontsize=16,cmap='tab20',normalize=False):\n",
    "\n",
    "    if classes is None:\n",
    "        classes=range(len(set(predict_classes)))\n",
    "\n",
    "    conf_matrix=confusion_matrix(Y_true, predict_classes, labels=classes)\n",
    "\n",
    "    if normalize:\n",
    "        total=conf_matrix.sum(axis=0) #summing actual predictions\n",
    "        conf_matrix= conf_matrix/total\n",
    "\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=classes,\n",
    "                  columns=classes)\n",
    "    #sns.set(font_scale=5)#for label size\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(df_cm, annot=True,annot_kws={\"size\":fontsize*(1/2) },cmap=cmap)# font size\n",
    "        #fig=conf_hm.get_figure()\n",
    "        plt.yticks(fontsize=fontsize+4)\n",
    "        plt.xticks(fontsize=fontsize+4)\n",
    "        plt.title(title, fontsize=fontsize+10)\n",
    "        #fig.savefig(out,dpi=300)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def ROC(y_true, y_pred,classes=None,title=None,plot=False):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    lw=4\n",
    "\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true=y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred=y_pred.values\n",
    "\n",
    "\n",
    "    if classes is None:\n",
    "        classes=set(list(y_true.flatten())) #removing dups, and i think itll only work for binary cases\n",
    "\n",
    "    if len(classes)>2:\n",
    "        #y_true = to_categorical(y_true, len(classes))\n",
    "        #y_pred = to_categorical(y_pred, len(classes))\n",
    "        \n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for i,cl in enumerate(classes):\n",
    "            fpr[cl], tpr[cl], _ = roc_curve(y_true[:,i], y_pred[:, i])\n",
    "            roc_auc[cl] = auc(fpr[cl], tpr[cl])\n",
    "        \n",
    "        if plot: \n",
    "            colors = plt.cm.get_cmap('tab20',len(classes)) #best line of code IN THE UNIVERSE\n",
    "\n",
    "            for i, color in zip(range(len(classes)), colors.colors):\n",
    "                cl = classes[i]\n",
    "                plt.plot(fpr[cl], tpr[cl], color=colors.colors[i], lw=lw,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(cl, roc_auc[cl]))\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "            plt.xlim([-0.03, 1.05])\n",
    "            plt.ylim([-0.03, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=10)\n",
    "            plt.ylabel('True Positive Rate',fontsize=10)\n",
    "            plt.title(title, fontsize=12)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.legend(loc=\"lower right\",prop={'size': 15})\n",
    "            #plt.savefig('/projects/sysbio/users/ianastop/cm_region_results/plots/%s_ROC.png'%self.out,dpi=300)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        return (fpr, tpr, roc_auc)\n",
    "\n",
    "    else:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        if plot:\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=4, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "            plt.xlim([-0.03, 1.05])\n",
    "            plt.ylim([-0.03, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=10)\n",
    "            plt.ylabel('True Positive Rate',fontsize=10)\n",
    "            plt.title(title, fontsize=12)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.legend(loc=\"lower right\",prop={'size': 10})\n",
    "            #plt.savefig('/projects/sysbio/users/ianastop/cm_region_results/plots/%s_ROC.png'%self.out,dpi=300)\n",
    "            return( fpr, tpr, roc_auc)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        print(fpr, tpr)\n",
    "        return fpr,tpr,roc_auc\n",
    "\n",
    "\n",
    "def regression_eval(y_true,y_pred, axis=None):\n",
    "    '''\n",
    "    Function computes pearson, r2 and rmse row wise or column wise\n",
    "    Returns a list for each metric for each row or column\n",
    "\n",
    "    axis =0/1 if the vector predicted has multiple rows and columns: for expression matrices\n",
    "    if the vectors has multiple samples predicted for 1 variable : for drug response vector of each drug individually\n",
    "    '''\n",
    "    #calculating metrics on a per sample basis\n",
    "    if axis ==0:\n",
    "        r_list = []\n",
    "        r2_list = []\n",
    "        rmse_list = []\n",
    "        for i, vec in enumerate(y_true):\n",
    "            r_list +=[pearsonr(vec, y_pred[i])[0]]\n",
    "            r2_list += [r2_score(vec, y_pred[i])]\n",
    "            rmse_list += [rmse(vec,y_pred[i])]\n",
    "        r = np.mean(r_list)\n",
    "        r2 = np.mean(r2_list)\n",
    "        rmse_score = np.mean(rmse_list)\n",
    "    elif axis == 1:\n",
    "        r_list = []\n",
    "        r2_list = []\n",
    "        rmse_list = []\n",
    "        for i, vec in enumerate(y_true.T):\n",
    "            r_list +=[pearsonr(vec, y_pred.T[i])[0]]\n",
    "            r2_list += [r2_score(vec, y_pred.T[i])]\n",
    "            rmse_list += [rmse(vec,y_pred.T[i])]\n",
    "        r = np.mean(r_list)\n",
    "        r2 = np.mean(r2_list)\n",
    "        rmse_score = np.mean(rmse_list)\n",
    "    elif axis is None:\n",
    "        r, _ = pearsonr(y_true.flatten(), y_pred.flatten())\n",
    "        r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "        rmse_score = rmse(y_true.flatten(), y_pred.flatten())\n",
    "    return r,r2,rmse_score\n",
    "\n",
    "def classification_eval(y_true,y_pred, y_proba,classes=None):\n",
    "    if len(y_proba.shape)>=2:\n",
    "        y_pred = np.argmax(y_proba, axis=1)\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = ROC(y_true, y_proba,classes=classes)[-1]\n",
    "    precision = average_precision_score(y_true, y_proba, average='weighted')\n",
    "    \n",
    "    return acc, f1, roc_auc, precision\n",
    "        \n",
    "def node_classification_eval():\n",
    "    pass \n",
    "\n",
    "def corr_plot(y_true, y_pred, title='Fig Title', ylabel='Y label', xlabel='X label', out_fn='out.png', log_values=False):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3,figsize=(10,10))\n",
    "    i=0\n",
    "    j=0\n",
    "    np.random.seed(42)\n",
    "    for idx,sample_idx in enumerate(np.random.choice(np.arange(y_true.shape[0]),9)):\n",
    "\n",
    "        ax_row =axes[i] #select row  to plot in\n",
    "        ax = ax_row[j]\n",
    "        if log_values:\n",
    "            observed = [np.log(x+1) for x in y_true[int(sample_idx/2)]]\n",
    "            predicted = [np.log(x+1) for x in y_pred[int(sample_idx/2)]]\n",
    "        else:\n",
    "            observed = y_true[int(sample_idx/2)]\n",
    "            predicted = y_pred[int(sample_idx/2)]\n",
    "            \n",
    "        ax.scatter(observed, predicted, s=1, alpha=0.6, marker='o')\n",
    "\n",
    "        ax.set_title(\"Random sample %d\"%(idx+1))\n",
    "\n",
    "        if j==2:\n",
    "            i+=1\n",
    "            j=0\n",
    "        else:\n",
    "            j+=1\n",
    "        r = pearsonr(y_true[int(sample_idx/2)], y_pred[int(sample_idx/2)])[0]\n",
    "        r2 = r2_score(y_true[int(sample_idx/2)], y_pred[int(sample_idx/2)])\n",
    "        ax.text(min(observed), 0.95*max(predicted), 'pearson coeff: %.3f'%r)\n",
    "        ax.text(min(observed), 0.8*max(predicted), r'$R^2$: %.3f'%r2)\n",
    "\n",
    "    #     handles, labels = ax.get_legend_handles_labels()\n",
    "    #     by_label = dict(zip(labels, handles))\n",
    "    #     ax.legend(by_label.values(), by_label.keys())\n",
    "        ax.set_ylabel(ylabel, fontsize=10)\n",
    "        ax.set_xlabel(xlabel, fontsize=10)\n",
    "\n",
    "    st = fig.suptitle(title, fontsize=15)\n",
    "    st.set_y(1.05)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_fn, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Treehouse Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "srp_tpm_df = pd.read_csv(\"../results/SRP132968_TPM_log2_plus_1.TH.balanced_maxdepth1_results.tsv\", index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srp_tpm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array([0]*srp_tpm_df.shape[0])\n",
    "pred_labels = srp_tpm_df.Ribo.values\n",
    "pred_proba = srp_tpm_df.Proba_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43444417, 0.43446213, 0.47162095, 0.4622694 , 0.48111457,\n",
       "       0.47141155, 0.48183977, 0.4346199 , 0.41511624, 0.48134945,\n",
       "       0.53618042, 0.40645406, 0.41574229, 0.4900444 , 0.43512117,\n",
       "       0.51764772, 0.5172776 , 0.46261475, 0.48995871, 0.4528199 ,\n",
       "       0.48055674, 0.44371442, 0.46218653, 0.45395426, 0.53652274,\n",
       "       0.45269134, 0.5364677 , 0.48042089, 0.52624669, 0.5365831 ,\n",
       "       0.49945312, 0.51778377, 0.44361009, 0.43438029, 0.41540904,\n",
       "       0.46189111, 0.55542097, 0.44353671, 0.42493   , 0.46214752,\n",
       "       0.46293105, 0.41597348, 0.48962185, 0.43391328, 0.43465002,\n",
       "       0.48038609, 0.51814799, 0.45291355, 0.44395159, 0.43404912,\n",
       "       0.38775208, 0.38783208, 0.49008453, 0.46103807, 0.40645406,\n",
       "       0.49980605, 0.41589791])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.01754386 0.92982456 0.96491228 1.        ] [nan nan nan nan nan]\n",
      "Accuracy: 0.8245614035087719\n",
      "F1-score: 0.9038461538461537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:681: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, f1, roc_auc, precision= classification_eval(true_labels,pred_labels,pred_proba, classes=[0,1])\n",
    "print('Accuracy: {}\\nF1-score: {}'.format(acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using refine.bio Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "srp_tpm_df = pd.read_csv(\"../results/SRP132968_TPM_log2_plus_1.balanced_maxdepth1_results.tsv\", index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srp_tpm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array([0]*srp_tpm_df.shape[0])\n",
    "pred_labels = srp_tpm_df.Ribo.values\n",
    "pred_proba = srp_tpm_df.Proba_1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.01754386 1.        ] [nan nan nan]\n",
      "Accuracy: 0.8245614035087719\n",
      "F1-score: 0.9038461538461537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, f1, roc_auc, precision= classification_eval(true_labels,pred_labels,pred_proba, classes=[0,1])\n",
    "print('Accuracy: {}\\nF1-score: {}'.format(acc, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
